{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bilstm_crf import BiLSTMCRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load vocab\n",
    "with open('data/vocab.txt', 'r') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "len(vocab)\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/process_data/train.json'\n",
    "DEV_PATH = 'data/process_data/dev.json'\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "train_sentences = list(train_data['text'].values())\n",
    "dev_sentences = list(dev_data['text'].values())\n",
    "\n",
    "train_labels = list(train_data['labels'].values())\n",
    "dev_labels = list(dev_data['labels'].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding\n",
    "embedding_maxtrix = np.load('embedding/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels is list of [start, end, tag#sentiment] for each sentence\n",
    "# convert to list of [start, end, tag] for each sentence to new form for train\n",
    "new_train_labels = []\n",
    "for i in range(len(train_labels)):\n",
    "\n",
    "    new_aspect_polarity = []\n",
    "    for aspect_plority in train_labels[i]:\n",
    "\n",
    "        start, end, tag_sentiment = aspect_plority\n",
    "        tag, sentiment = tag_sentiment.split('#')\n",
    "        new_aspect_polarity.append([start, end, tag])\n",
    "    new_train_labels.append(new_aspect_polarity)\n",
    "\n",
    "# convert to list of [start, end, tag] for each sentence to new form for dev\n",
    "new_dev_labels = []\n",
    "for i in range(len(dev_labels)):\n",
    "\n",
    "    new_aspect_polarity = []\n",
    "    for aspect_plority in dev_labels[i]:\n",
    "\n",
    "        start, end, tag_sentiment = aspect_plority\n",
    "        tag, sentiment = tag_sentiment.split('#')\n",
    "        new_aspect_polarity.append([start, end, tag])\n",
    "    new_dev_labels.append(new_aspect_polarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert data to ids\n",
    "def convert_to_ids(data, vocab, max_len=256):\n",
    "    id_data = []\n",
    "\n",
    "    pad_token_id = vocab.index('<PAD>')\n",
    "    ukn_token_id = vocab.index('<UNK>')\n",
    "    for sentence in data:\n",
    "        ids = []\n",
    "        for word in sentence.split():\n",
    "            if word in vocab:\n",
    "                ids.append(vocab.index(word))\n",
    "            else:\n",
    "                ids.append(ukn_token_id)\n",
    "\n",
    "        if len(ids) < max_len:\n",
    "            ids += [pad_token_id] * (max_len - len(ids))\n",
    "        id_data.append(np.array(ids))\n",
    "        \n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = convert_to_ids(train_sentences, vocab)\n",
    "dev_tokenized = convert_to_ids(dev_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [torch.LongTensor(tokenized) for tokenized in train_tokenized]\n",
    "dev_tokenized = [torch.LongTensor(tokenized) for tokenized in dev_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model = BiLSTMCRF(vocab_size=len(vocab), tag_to_ix=tag_to_id, hidden_dim=200, embedding_maxtrix=embedding_maxtrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = train_tokenized[0]\n",
    "feats = span_detection_model._get_lstm_features(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\train.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m label \u001b[39m=\u001b[39m new_train_labels[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m span_detection_model\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m span_detection_model\u001b[39m.\u001b[39;49mneg_log_likelihood(sentence, label)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train.ipynb#X14sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\bilstm_crf.py:67\u001b[0m, in \u001b[0;36mBiLSTMCRF.neg_log_likelihood\u001b[1;34m(self, sentence, tags)\u001b[0m\n\u001b[0;32m     64\u001b[0m feats \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lstm_features(sentence)\n\u001b[0;32m     66\u001b[0m \u001b[39m# forward_score: (batch_size)\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m forward_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcrf\u001b[39m.\u001b[39;49mforward(feats, tags)\n\u001b[0;32m     69\u001b[0m \u001b[39m# gold_score: (batch_size)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m gold_score \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcrf\u001b[39m.\u001b[39mscore_sentence(feats, tags)\n",
      "File \u001b[1;32mc:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\TorchCRF\\__init__.py:90\u001b[0m, in \u001b[0;36mCRF.forward\u001b[1;34m(self, emissions, tags, mask, reduction)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     64\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[0;32m     65\u001b[0m         emissions: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m         reduction: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     69\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m     70\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute the conditional log likelihood of a sequence of tags given emission scores.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \n\u001b[0;32m     72\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39m        reduction is ``none``, ``()`` otherwise.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate(emissions, tags\u001b[39m=\u001b[39;49mtags, mask\u001b[39m=\u001b[39;49mmask)\n\u001b[0;32m     91\u001b[0m     \u001b[39mif\u001b[39;00m reduction \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mtoken_mean\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     92\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39minvalid reduction: \u001b[39m\u001b[39m{\u001b[39;00mreduction\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\TorchCRF\\__init__.py:154\u001b[0m, in \u001b[0;36mCRF._validate\u001b[1;34m(self, emissions, tags, mask)\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    150\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mexpected last dimension of emissions is \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_tags\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    151\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00memissions\u001b[39m.\u001b[39msize(\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m tags \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[39mif\u001b[39;00m emissions\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m] \u001b[39m!=\u001b[39m tags\u001b[39m.\u001b[39;49mshape:\n\u001b[0;32m    155\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    156\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mthe first two dimensions of emissions and tags must match, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    157\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(emissions\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(tags\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    159\u001b[0m \u001b[39mif\u001b[39;00m mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# train\n",
    "optimizer = optim.SGD(span_detection_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "epoch_num = 10\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print('epoch: {}'.format(epoch))\n",
    "    for i in range(len(train_tokenized)):\n",
    "        sentence = train_tokenized[i]\n",
    "        label = new_train_labels[i]\n",
    "        span_detection_model.zero_grad()\n",
    "        loss = span_detection_model.neg_log_likelihood(sentence, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('loss: {}'.format(loss.item()))\n",
    "\n",
    "    # dev test\n",
    "    # with torch.no_grad():\n",
    "    #     for i in range(len(dev_sentences)):\n",
    "    #         sentence = dev_sentences[i]\n",
    "    #         label = dev_labels[i]\n",
    "    #         span_detection_model.zero_grad()\n",
    "    #         loss = span_detection_model.neg_log_likelihood(sentence, label)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         if i % 100 == 0:\n",
    "    #             print('loss: {}'.format(loss.item()))\n",
    "\n",
    "# save model\n",
    "# torch.save(span_detection_model.state_dict(), 'model/span_detection_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
