{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bilstm_crf import BiLSTMCRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load embedding\n",
    "embedding_maxtrix = np.load('embedding/embedding_matrix.npy')\n",
    "\n",
    "# load vocab\n",
    "with open('data/vocab.txt', 'r') as f:\n",
    "    vocab = f.read().split('\\n')\n",
    "len(vocab)\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/span_detection_datasets_IOB/train.json'\n",
    "DEV_PATH = 'data/span_detection_datasets_IOB/dev.json'\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "train_sentences = list(train_data['text'].values())\n",
    "dev_sentences = list(dev_data['text'].values())\n",
    "\n",
    "train_labels = list(train_data['labels'].values())\n",
    "dev_labels = list(dev_data['labels'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert data to ids\n",
    "def convert_to_ids(data, vocab, max_len=256):\n",
    "    id_data = []\n",
    "\n",
    "    pad_token_id = vocab.index('<PAD>')\n",
    "    ukn_token_id = vocab.index('<UNK>')\n",
    "    for sentence in data:\n",
    "        ids = []\n",
    "        for word in sentence.split():\n",
    "            if word in vocab:\n",
    "                ids.append(vocab.index(word))\n",
    "            else:\n",
    "                ids.append(ukn_token_id)\n",
    "\n",
    "        if len(ids) < max_len:\n",
    "            ids += [pad_token_id] * (max_len - len(ids))\n",
    "        id_data.append(np.array(ids))\n",
    "        \n",
    "    return id_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = convert_to_ids(train_sentences, vocab)\n",
    "dev_tokenized = convert_to_ids(dev_sentences, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = [torch.LongTensor(tokenized) for tokenized in train_tokenized]\n",
    "dev_tokenized = [torch.LongTensor(tokenized) for tokenized in dev_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# Convert labels to ids\n",
    "# labels = [[start, end, tag], ...]\n",
    "def convert_labels_to_ids(label, tag_to_id, max_len=256):\n",
    "    ids = [tag_to_id[tag] for tag in label]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [tag_to_id['O']] * (max_len - len(ids))\n",
    "        \n",
    "    return np.array(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in train_labels])\n",
    "dev_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in dev_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_detection_model = BiLSTMCRF(vocab_size=len(vocab), tag_to_ix=tag_to_id, hidden_dim=200, embedding_maxtrix=embedding_maxtrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "optimizer = optim.SGD(span_detection_model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "epoch_num = 10\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    print('epoch: {}'.format(epoch))\n",
    "    for i in range(len(train_tokenized)):\n",
    "        sentence = train_tokenized[i]\n",
    "        label = train_labels_encoding[i]\n",
    "        span_detection_model.zero_grad()\n",
    "        loss = span_detection_model.neg_log_likelihood(sentence, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 100 == 0:\n",
    "            print('loss: {}'.format(loss.item()))\n",
    "\n",
    "    # dev test\n",
    "    # with torch.no_grad():\n",
    "    #     for i in range(len(dev_sentences)):\n",
    "    #         sentence = dev_sentences[i]\n",
    "    #         label = dev_labels_encoding[i]\n",
    "    #         span_detection_model.zero_grad()\n",
    "    #         loss = span_detection_model.neg_log_likelihood(sentence, label)\n",
    "    #         loss.backward()\n",
    "    #         optimizer.step()\n",
    "    #         if i % 100 == 0:\n",
    "    #             print('loss: {}'.format(loss.item()))\n",
    "\n",
    "# save model\n",
    "# torch.save(span_detection_model.state_dict(), 'model/span_detection_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
