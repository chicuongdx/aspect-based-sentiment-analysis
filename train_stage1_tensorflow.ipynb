{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from bilstm_crf import build_bilstm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')\n",
    "\n",
    "# load vocab\n",
    "# with open('data/vocab.txt', 'r') as f:\n",
    "#     vocab = f.read().split('\\n')\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/span_detection_datasets_IOB/train.json'\n",
    "DEV_PATH = 'data/span_detection_datasets_IOB/dev.json'\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "train_sentences = list(train_data['text'].values())\n",
    "dev_sentences = list(dev_data['text'].values())\n",
    "\n",
    "train_labels = list(train_data['labels'].values())\n",
    "dev_labels = list(dev_data['labels'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "tokenizer = TextVectorization(pad_to_max_tokens=True,\n",
    "                              output_sequence_length=256,\n",
    "                              output_mode='int',\n",
    "                              max_tokens=10000)\n",
    "tokenizer.adapt(train_sentences + dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab and save it to file vocab.txt\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "with open('data/vocab.txt', 'w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokenized and dev_tokenized are numpy array with padding\n",
    "train_tokenized = tokenizer(np.array([[s] for s in train_sentences])).numpy()\n",
    "dev_tokenized = tokenizer(np.array([[s] for s in dev_sentences])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_labels_to_ids(label, tag_to_id, max_len=256):\n",
    "    ids = [int(tag_to_id[tag]) for tag in label]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [int(tag_to_id['<PAD>'])] * (max_len - len(ids))\n",
    "        \n",
    "    return np.array(ids, dtype=np.int32)\n",
    "\n",
    "train_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in train_labels], dtype=np.int32)\n",
    "dev_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in dev_labels], dtype=np.int32)\n",
    "\n",
    "# change label (batch_size, max_len) to (batch_size, max_len, 1)\n",
    "# train_labels_encoding = np.expand_dims(train_labels_encoding, axis=-1)\n",
    "# dev_labels_encoding = np.expand_dims(dev_labels_encoding, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "train_labels_one_hot_encoding = tf.one_hot(train_labels_encoding, len(tag_to_id))\n",
    "dev_labels_one_hot_encoding = tf.one_hot(dev_labels_encoding, len(tag_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader tensorflow\n",
    "BATCH_SIZE = 2\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tokenized, train_labels_one_hot_encoding))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_tokenized, dev_labels_one_hot_encoding))\n",
    "dev_dataset = dev_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "\n",
    "# # Load the pre-trained model\n",
    "# embedding_model = fasttext.load_model('pretrained-weights/cc.vi.300.bin')\n",
    "\n",
    "# vocabulary = tokenizer.get_vocabulary()\n",
    "# vector_dim = embedding_model.get_dimension()\n",
    "\n",
    "# embedding_matrix = np.zeros((len(vocabulary), vector_dim))\n",
    "# for i, word in enumerate(vocabulary):\n",
    "#         embedding_matrix[i] = embedding_model.get_word_vector(word)\n",
    "\n",
    "# embedding_matrix_file = 'embedding/embedding_matrix.npy'\n",
    "\n",
    "# np.save(embedding_matrix_file, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BiLSTM_CRF(Model):\n",
    "#     def __init__(self, vocab_size, max_len, n_tags, embedding_matrix=None, embedding_dim=None, unit='lstm', num_units=100, dropout=0.1, recurrent_dropout=0.1):\n",
    "#         super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#         if embedding_matrix is not None and embedding_dim is not None:\n",
    "#             raise ValueError('Cannot provide both an embedding matrix and an embedding dimension.')\n",
    "\n",
    "#         if embedding_matrix is not None:\n",
    "#             self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[-1], input_length=max_len, mask_zero=True, weights=[embedding_matrix], trainable=False, name='embedding')\n",
    "#         elif embedding_dim is not None:\n",
    "#             self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True, embeddings_initializer='uniform', name='embedding')\n",
    "#         else:\n",
    "#             raise ValueError('Must provide either an embedding matrix or an embedding dimension.')\n",
    "\n",
    "#         if unit == 'lstm':\n",
    "#             self.lstm = layers.Bidirectional(layers.LSTM(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='bilstm')\n",
    "#         elif unit == 'gru':\n",
    "#             self.lstm = layers.Bidirectional(layers.GRU(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='bigru')\n",
    "#         elif unit == 'rnn':\n",
    "#             self.lstm = layers.Bidirectional(layers.SimpleRNN(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='birnn')\n",
    "#         else:\n",
    "#             raise ValueError('Invalid unit type. Must be one of lstm, gru, or rnn.')\n",
    "        \n",
    "#         self.dropout = layers.Dropout(dropout, name='dropout')\n",
    "#         self.time_distributed = layers.TimeDistributed(layers.Dense(n_tags, activation=\"relu\"), name='time_distributed')\n",
    "        \n",
    "#         self.crf = CRF(units=n_tags, name='crf')\n",
    "\n",
    "#     def call(self, inputs, training=False):\n",
    "\n",
    "#         x = self.embedding(inputs)\n",
    "#         x = self.lstm(x)\n",
    "#         x = self.dropout(x, training=training)\n",
    "#         x = self.time_distributed(x)\n",
    "#         decoded_sequence, potentials, sequence_length, chain_kernel = self.crf(x)\n",
    "\n",
    "#         return potentials\n",
    "    \n",
    "#     def summary(self):\n",
    "#         x = layers.Input(shape=(self.max_len,), name='input')\n",
    "#         model = Model(inputs=[x], outputs=self.call(x))\n",
    "#         return model.summary()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_bilstm_crf(vocab_size, max_len, n_tags, embedding_matrix=None, embedding_dim=None, unit='lstm', num_units=100, dropout=0.1, recurrent_dropout=0.1):\n",
    "    \n",
    "    inputs = layers.Input(shape=(max_len,), name='input')\n",
    "    \n",
    "    if embedding_matrix is not None and embedding_dim is not None:\n",
    "        raise ValueError('Cannot provide both an embedding matrix and an embedding dimension.')\n",
    "\n",
    "    if embedding_matrix is not None:\n",
    "        embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[-1], input_length=max_len, mask_zero=True, weights=[embedding_matrix], trainable=False, name='embedding')\n",
    "    elif embedding_dim is not None:\n",
    "        embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True, embeddings_initializer='uniform', name='embedding')\n",
    "    else:\n",
    "        raise ValueError('Must provide either an embedding matrix or an embedding dimension.')\n",
    "\n",
    "    if unit == 'lstm':\n",
    "        lstm = layers.Bidirectional(layers.LSTM(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='bilstm')\n",
    "    elif unit == 'gru':\n",
    "        lstm = layers.Bidirectional(layers.GRU(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='bigru')\n",
    "    elif unit == 'rnn':\n",
    "        lstm = layers.Bidirectional(layers.SimpleRNN(units=num_units, return_sequences=True, dropout=recurrent_dropout), name='birnn')\n",
    "    else:\n",
    "        raise ValueError('Invalid unit type. Must be one of lstm, gru, or rnn.')\n",
    "        \n",
    "    dropout_model = layers.Dropout(dropout, name='dropout')\n",
    "    time_distributed = layers.TimeDistributed(layers.Dense(n_tags, activation=\"relu\"), name='time_distributed')\n",
    "        \n",
    "    crf = CRF(units=n_tags, name='crf')\n",
    "\n",
    "    x = embedding(inputs)\n",
    "    x = lstm(x)\n",
    "    x = dropout_model(x)\n",
    "    x = time_distributed(x)\n",
    "    decoded_sequence, potentials, sequence_length, chain_kernel = crf(x)\n",
    "\n",
    "    model = Model([inputs], potentials, name='bilstm_crf')\n",
    "\n",
    "    model.add_loss(tf.abs(tf.reduce_mean(chain_kernel)))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"bilstm_crf\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 256)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 256, 300)          2420400   \n",
      "                                                                 \n",
      " bilstm (Bidirectional)      (None, 256, 200)          320800    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256, 200)          0         \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 256, 22)           4422      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 256),             1034      \n",
      "                              (None, 256, 22),                   \n",
      "                              (None,),                           \n",
      "                              (22, 22)]                          \n",
      "                                                                 \n",
      " tf.math.reduce_mean_6 (TFO  ()                        0         \n",
      " pLambda)                                                        \n",
      "                                                                 \n",
      " tf.math.abs_6 (TFOpLambda)  ()                        0         \n",
      "                                                                 \n",
      " add_loss_6 (AddLoss)        ()                        0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2746656 (10.48 MB)\n",
      "Trainable params: 2746656 (10.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "VOCAB_SIZE = len(vocab)\n",
    "TAG_SIZE = len(tag_to_id)\n",
    "UNITS = 100\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Build model\n",
    "model = build_model_bilstm_crf(vocab_size=VOCAB_SIZE, max_len=MAX_LEN, n_tags=TAG_SIZE, embedding_dim=EMBEDDING_DIM, unit='lstm', num_units=UNITS, dropout=0.1, recurrent_dropout=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:The following Variables were used in a Lambda layer's call (tf.math.reduce_mean_6), but are not present in its tracked objects:   <tf.Variable 'chain_kernel:0' shape=(22, 22) dtype=float32>. This is a strong indication that the Lambda layer should be rewritten as a subclassed Layer.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 316, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 2 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sigmoid_focal_crossentropy/weighted_loss/Mul, bilstm_crf/tf.math.abs_6/Abs)' with input shapes: [2,256], [].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\train_stage1_tensorflow.ipynb Cell 23\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m                     epochs\u001b[39m=\u001b[39;49mEPOCHS,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m                     validation_data\u001b[39m=\u001b[39;49mdev_dataset,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[early_stopping]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_tensorflow.ipynb#X31sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m                     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_filexfxh1upn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 316, in __call__\n        total_total_loss_mean_value = tf.add_n(total_loss_mean_values)\n\n    ValueError: Shapes must be equal rank, but are 2 and 0\n    \tFrom merging shape 0 with other shapes. for '{{node AddN}} = AddN[N=2, T=DT_FLOAT](sigmoid_focal_crossentropy/weighted_loss/Mul, bilstm_crf/tf.math.abs_6/Abs)' with input shapes: [2,256], [].\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "# loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "# loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "loss = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=0.01)\n",
    "\n",
    "metric_acc = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# metric_f1 = tf.keras.metrics.F1Score(average='micro')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric_acc])\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# train model\n",
    "EPOCHS = 10\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=dev_dataset,\n",
    "                    callbacks=[early_stopping]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy of train and dev in one figure\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[0].set_title('Model loss')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['Train', 'Val'], loc='upper right')\n",
    "\n",
    "    axs[1].plot(history.history['categorical_accuracy'])\n",
    "    axs[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axs[1].set_title('Model accuracy')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['Train', 'Val'], loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model/span_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
