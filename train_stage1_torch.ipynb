{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/process_data_split_words/train.jsonl'\n",
    "TEST_PATH = 'data/process_data_split_words/test.jsonl'\n",
    "DEV_PATH = 'data/process_data_split_words/dev.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function read jsonl file as dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')\n",
    "\n",
    "# load vocab\n",
    "# with open('data/vocab.txt', 'r') as f:\n",
    "#     vocab = f.read().split('\\n')\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/span_detection_datasets_split_word_IOB/train.jsonl'\n",
    "DEV_PATH = 'data/span_detection_datasets_split_word_IOB/dev.jsonl'\n",
    "\n",
    "train_data = read_jsonl_to_dataframe(TRAIN_PATH)\n",
    "dev_data = read_jsonl_to_dataframe(DEV_PATH)\n",
    "\n",
    "\n",
    "train_sentences = list(train_data.text.apply(lambda x: \" \".join(x)))\n",
    "dev_sentences = list(dev_data.text.apply(lambda x: \" \".join(x)))\n",
    "\n",
    "train_labels = list(train_data.labels)\n",
    "dev_labels = list(dev_data.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH_TOKEN = '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'\n",
    "TOKENIZER_PATH = 'nguyenvulebinh/vi-mrc-large'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "In this solution we use pretrained tokenizer from [HuggingFace](https://huggingface.co/nguyenvulebinh/vi-mrc-large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH, use_fast=True)\n",
    "\n",
    "class SpanDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, sentences, labels, tag_to_id, tokenizer, max_len=MAX_LEN):\n",
    "\n",
    "        self.sentences = sentences\n",
    "        self.labels = labels\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # encode all sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoded_sentences = self.tokenizer.batch_encode_plus(\n",
    "            self.sentences,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            max_length=self.max_len,\n",
    "        )\n",
    "\n",
    "        # tags to ids\n",
    "        self.tag_to_id = tag_to_id\n",
    "        self.encoded_labels = self.convert_labels_to_ids()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            'input_ids': self.encoded_sentences['input_ids'][index],\n",
    "            'attention_mask': self.encoded_sentences['attention_mask'][index],\n",
    "            'labels': self.encoded_labels[index]\n",
    "        }\n",
    "    \n",
    "    def convert_labels_to_ids(self):\n",
    "        encoded_labels = []\n",
    "        for label in self.labels:\n",
    "            ids = [int(self.tag_to_id[tag]) for tag in label]\n",
    "\n",
    "            if len(ids) < self.max_len:\n",
    "                ids += [int(self.tag_to_id['<PAD>'])] * (self.max_len - len(ids))\n",
    "                \n",
    "            encoded_labels.append(torch.tensor(ids, dtype=torch.long, device=device))\n",
    "                \n",
    "        return encoded_labels\n",
    "        \n",
    "\n",
    "train_dataset = SpanDetectionDataset(train_sentences, train_labels, tag_to_id, tokenizer)\n",
    "dev_dataset = SpanDetectionDataset(dev_sentences, dev_labels, tag_to_id, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader tensorflow\n",
    "BATCH_SIZE = 2\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_dataloader = torch.utils.data.DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fasttext\n",
    "\n",
    "# # Load the pre-trained model\n",
    "# embedding_model = fasttext.load_model('pretrained-weights/cc.vi.300.bin')\n",
    "\n",
    "# vocabulary = tokenizer.get_vocabulary()\n",
    "# vector_dim = embedding_model.get_dimension()\n",
    "\n",
    "# embedding_matrix = np.zeros((len(vocabulary), vector_dim))\n",
    "# for i, word in enumerate(vocabulary):\n",
    "#         embedding_matrix[i] = embedding_model.get_word_vector(word)\n",
    "\n",
    "# embedding_matrix_file = 'embedding/embedding_matrix.npy'\n",
    "\n",
    "# np.save(embedding_matrix_file, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span detection model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "    def __init__(self, vocab_size, tag_to_id, batch_size, embedding_matrix=None, embedding_dim=None, hidden_dim=200, units='lstm', droput=0.2, recurrent_dropout=0.2, max_len=MAX_LEN):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_id\n",
    "        self.tagset_size = len(tag_to_id)\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # check embedding matrix and embedding dimension\n",
    "        if embedding_matrix is None and embedding_dim is None:\n",
    "            raise ValueError('You must provide either embedding matrix or embedding dimension')\n",
    "        if embedding_matrix is not None and embedding_dim is not None:\n",
    "            raise ValueError('You must provide either embedding matrix or embedding dimension, not both')\n",
    "        \n",
    "        if embedding_matrix is None:\n",
    "            self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            self.word_embeds = nn.Embedding.from_pretrained(torch.FloatTensor(embedding_matrix))\n",
    "\n",
    "        num_layers = 1\n",
    "        if units == 'lstm':\n",
    "            self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2, bidirectional=True, dropout=recurrent_dropout, num_layers=num_layers)\n",
    "        elif units == 'gru':\n",
    "            self.lstm = nn.GRU(embedding_dim, hidden_dim // 2, bidirectional=True, dropout=recurrent_dropout, num_layers=num_layers)\n",
    "        elif units == 'rnn':\n",
    "            self.lstm = nn.RNN(embedding_dim, hidden_dim // 2, bidirectional=True, dropout=recurrent_dropout, num_layers=num_layers)\n",
    "        else:\n",
    "            raise ValueError('Invalid unit type, must be one of \"lstm\", \"gru\", \"rnn\"')\n",
    "        \n",
    "        self.hidden2tag = nn.Linear(hidden_dim, len(self.tag_to_ix))\n",
    "\n",
    "        # self.dropout = nn.Dropout(droput)\n",
    "\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size).to(device)\n",
    "            )\n",
    "\n",
    "        self.transitions.data[tag_to_id[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_id[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, self.max_len, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2, self.max_len, self.hidden_dim // 2).to(device))\n",
    "    \n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "    \n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        # embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        embeds = self.word_embeds(sentence)\n",
    "\n",
    "        # print(embeds.shape)\n",
    "        # print(self.hidden[0].shape, self.hidden[1].shape)\n",
    "        \n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "\n",
    "        # print(lstm_out.shape, self.hidden[0].shape, self.hidden[1].shape)\n",
    "        \n",
    "        # lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "\n",
    "        # lstm_feats = self.dropout(lstm_feats)\n",
    "        return lstm_feats\n",
    "    \n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1, device=device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long, device=device), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, batch_sentence, tags):\n",
    "        batch_feats = self._get_lstm_features(batch_sentence) #[batch_size, max_len, hidden_dim/2]\n",
    "\n",
    "        batch_forward_score = torch.zeros(1, device=device)\n",
    "        batch_gold_score = torch.zeros(1, device=device)\n",
    "        \n",
    "        for feats, tag in zip(batch_feats, tags):\n",
    "            forward_score = self._forward_alg(feats) # this function get input is [max_len, hidden_dim/2]\n",
    "            gold_score = self._score_sentence(feats, tag)\n",
    "\n",
    "            batch_forward_score += forward_score\n",
    "            batch_gold_score += gold_score\n",
    "\n",
    "        return batch_forward_score - batch_gold_score\n",
    "    \n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        batch_score, batch_tag_seq = torch.zeros(self.batch_size, device=device), torch.zeros(self.batch_size, device=device)\n",
    "        \n",
    "        for feats in lstm_feats:\n",
    "            score, tag_seq = self._viterbi_decode(feats)\n",
    "            batch_score += score\n",
    "            batch_tag_seq += tag_seq\n",
    "\n",
    "        return batch_score, batch_tag_seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\absa\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\train_stage1_torch.ipynb Cell 20\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m         scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m best_model, train_loss, dev_loss\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m model, train_loss, dev_loss \u001b[39m=\u001b[39m train_model((train_dataloader, dev_dataloader), epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m, weight_decay\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, early_stopping\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)\n",
      "\u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\train_stage1_torch.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mneg_log_likelihood(input_ids, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/train_stage1_torch.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Hii\\miniconda3\\envs\\absa\\Lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Hii\\miniconda3\\envs\\absa\\Lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m     tensors,\n\u001b[0;32m    253\u001b[0m     grad_tensors_,\n\u001b[0;32m    254\u001b[0m     retain_graph,\n\u001b[0;32m    255\u001b[0m     create_graph,\n\u001b[0;32m    256\u001b[0m     inputs,\n\u001b[0;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    259\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_model(dataloader, epochs=20, lr=0.01, weight_decay=0.01, early_stopping=5):\n",
    "    train_loader, dev_loader = dataloader\n",
    "\n",
    "    model = BiLSTM_CRF(vocab_size=len(tokenizer.get_vocab()), tag_to_id=tag_to_id, batch_size=BATCH_SIZE, embedding_dim=300)\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = model.neg_log_likelihood(input_ids, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        dev_loss = 0\n",
    "        for batch in dev_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            loss = model.neg_log_likelihood(input_ids, labels)\n",
    "\n",
    "            dev_loss += loss.item()\n",
    "        \n",
    "        dev_loss /= len(dev_loader)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, train_loss: {train_loss}, dev_loss: {dev_loss}')\n",
    "\n",
    "        # calculate F1 score\n",
    "\n",
    "        # early stopping\n",
    "        if dev_loss < best_loss:\n",
    "            best_loss = dev_loss\n",
    "            best_model = model\n",
    "            early_stopping_counter = 0\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "        \n",
    "        if early_stopping_counter >= early_stopping:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "    return best_model, train_loss, dev_loss\n",
    "\n",
    "model, train_loss, dev_loss = train_model((train_dataloader, dev_dataloader), epochs=20, lr=0.001, weight_decay=0.01, early_stopping=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy of train and dev in one figure\n",
    "def plot_loss_and_accuracy(train_loss, dev_loss, train_acc, dev_acc):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 6))\n",
    "\n",
    "    ax1.plot(train_loss, label='train_loss')\n",
    "    ax1.plot(dev_loss, label='dev_loss')\n",
    "    ax1.set_title('Loss')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(train_acc, label='train_acc')\n",
    "    ax2.plot(dev_acc, label='dev_acc')\n",
    "    ax2.set_title('Accuracy')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model/span_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
