{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PATH = 'data/process_data_final/train.json'\n",
    "# DEV_PATH = 'data/process_data_final/dev.json'\n",
    "# TEST_PATH = 'data/process_data_final/test.json'\n",
    "\n",
    "TRAIN_PATH = 'data/span_detection_datasets_IOB/train.json'\n",
    "DEV_PATH = 'data/span_detection_datasets_IOB/dev.json'\n",
    "TEST_PATH = 'data/span_detection_datasets_IOB/test.json'\n",
    "\n",
    "AUTH_TOKEN = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_json(TRAIN_PATH)\n",
    "dev_df = pd.read_json(DEV_PATH)\n",
    "test_df = pd.read_json(TEST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # load the tokenizer for the model BiLSTM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('nguyenvulebinh/vi-mrc-large', use_auth_token=AUTH_TOKEN)\n",
    "\n",
    "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# def tokenize_sentences(sentences, tokenizer, max_length=384):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "\n",
    "#     # For every sentence...\n",
    "#     for sent in sentences:\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                             sent,                      # Sentence to encode.\n",
    "#                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                             max_length = max_length,           # Pad & truncate all sentences.\n",
    "#                             pad_to_max_length = True,\n",
    "#                             return_attention_mask = True,   # Construct attn. masks.\n",
    "#                             return_tensors = 'pt',\n",
    "#                             truncation=True\n",
    "#                        )\n",
    "        \n",
    "#         # Add the encoded sentence to the list.    \n",
    "#         input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "#         # And its attention mask (simply differentiates padding from non-padding).\n",
    "#         attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#     return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df['text'].values\n",
    "dev_sentences = dev_df['text'].values\n",
    "test_sentences = test_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# train_input_ids, train_attention_masks = tokenize_sentences(train_sentences, tokenizer)\n",
    "# dev_input_ids, dev_attention_masks = tokenize_sentences(dev_sentences, tokenizer)\n",
    "# test_input_ids, test_attention_masks = tokenize_sentences(test_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data\n",
    "# import torch\n",
    "\n",
    "# torch.save(train_input_ids, 'data/tokenizer_data/train_input_ids.pt')\n",
    "# torch.save(train_attention_masks, 'data/tokenizer_data/train_attention_masks.pt')\n",
    "# torch.save(dev_input_ids, 'data/tokenizer_data/dev_input_ids.pt')\n",
    "# torch.save(dev_attention_masks, 'data/tokenizer_data/dev_attention_masks.pt')\n",
    "# torch.save(test_input_ids, 'data/tokenizer_data/test_input_ids.pt')\n",
    "# torch.save(test_attention_masks, 'data/tokenizer_data/test_attention_masks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vocab from train data, dev data and test data\n",
    "vocab = set()\n",
    "for sent in np.concatenate([train_sentences, dev_sentences, test_sentences]):\n",
    "    vocab.update(sent.split())\n",
    "for sent in dev_sentences:\n",
    "    vocab.update(sent.split())\n",
    "for sent in test_sentences:\n",
    "    vocab.update(sent.split())\n",
    "vocab = list(vocab)\n",
    "vocab.sort()\n",
    "\n",
    "# insert special tokens to first index\n",
    "vocab.insert(0, '<UNK>')\n",
    "vocab.insert(1, '<PAD>')\n",
    "\n",
    "\n",
    "import json\n",
    "# Save vocab as a txt file\n",
    "with open('data/vocab.txt', 'w', encoding='utf-8') as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['labels'].values\n",
    "train_dev_labels = dev_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag_to_id\n",
    "tag_to_id = {}\n",
    "tag_to_id['O'] = 1\n",
    "for label in train_labels:\n",
    "\n",
    "    for span in label:\n",
    "\n",
    "        if span == 'O':\n",
    "            continue\n",
    "        \n",
    "        aspect = span\n",
    "\n",
    "        if aspect not in tag_to_id:\n",
    "            tag_to_id[aspect] = len(tag_to_id) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tag_to_id\n",
    "import json\n",
    "\n",
    "with open('data/tag_to_id.json', 'w') as f:\n",
    "    json.dump(tag_to_id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
