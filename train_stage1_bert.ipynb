{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\zac2023\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModel, AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "MAX_LEN = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \".env\"\n",
    "\n",
    "try:\n",
    "    with open(env, \"r\") as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {env} does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "MRC_PATH = 'nguyenvulebinh/vi-mrc-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/span_detection_datasets_split_word_IOB/train.jsonl'\n",
    "DEV_PATH = 'data/span_detection_datasets_split_word_IOB/dev.jsonl'\n",
    "TEST_PATH = 'data/span_detection_datasets_split_word_IOB/test.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function read jsonl file as dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id_bert.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# convert tag_to_id to id_to_tag\n",
    "id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
    "\n",
    "# load train and dev data\n",
    "\n",
    "df_train = read_jsonl_to_dataframe(TRAIN_PATH)\n",
    "df_dev = read_jsonl_to_dataframe(DEV_PATH)\n",
    "\n",
    "df_train.text = df_train.text.apply(lambda x: \" \".join(x))\n",
    "df_dev.text = df_dev.text.apply(lambda x: \" \".join(x))\n",
    "\n",
    "\n",
    "# train_sentences = list(df_train.text)\n",
    "# dev_sentences = list(df_dev.text)\n",
    "\n",
    "# train_labels = list(df_train.labels)\n",
    "# dev_labels = list(df_dev.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\zac2023\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MRC_PATH, use_auth_token=AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁pin', '▁s', 'à', 'i', '_', 't', 'ầ', 'm', '▁50', '▁h', '▁cho', '▁pin', '▁100', '▁100', '▁camera', '▁ổn', '▁tất', '_', 'c', 'ả', '▁đều', '▁ok', '▁nhân', '_', 'vi', 'ên', '▁thế', '_', 'gi', 'ới', '▁di', '_', 'đ', 'ộ', 'ng', '▁trần', '_', 'vă', 'n', '_', 'th', 'ời', '▁cà', '_', 'ma', 'u', '▁nhiệt', '_', 't', 'ình', '▁và', '▁vui', '_', 'v', 'ẻ', '▁chúc', '▁các', '▁a', 'e', '▁sức', '▁khỏe', '▁tốt', '▁và', '▁phục', '_', 'ok', '▁h', 'oài', '_', 'n', 'ha']\n",
      "['B-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'I-BATTERY', 'B-CAMERA', 'I-CAMERA', 'B-GENERAL', 'B-GENERAL', 'B-GENERAL', 'B-GENERAL', 'I-GENERAL', 'I-GENERAL', 'B-SER&ACC', 'B-SER&ACC', 'B-SER&ACC', 'B-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC', 'I-SER&ACC']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "\n",
    "sample = df_train.text.iloc[0]\n",
    "sample_label = df_train.labels.iloc[0]\n",
    "\n",
    "tokenized_sentence, labels = tokenize_and_preserve_labels(sample, sample_label, tokenizer)\n",
    "print(tokenized_sentence)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define special tokens\n",
    "pad_token = tokenizer.pad_token\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanDetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=MAX_LEN):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.text[index]  \n",
    "        word_labels = self.data.labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [cls_token] + tokenized_sentence + [sep_token] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + [pad_token for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != pad_token else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [tag_to_id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create train and dev dataset\n",
    "train_dataset = SpanDetectionDataset(df_train, tokenizer)\n",
    "dev_dataset = SpanDetectionDataset(df_dev, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>         O\n",
      "▁lag        B-PERFORMANCE\n",
      "▁va         O\n",
      "▁hao        B-BATTERY\n",
      "▁pin        I-BATTERY\n",
      "▁là         O\n",
      "▁cái        O\n",
      "▁tóm        O\n",
      "_           O\n",
      "t           O\n",
      "ắ           O\n",
      "t           O\n",
      "▁về         O\n",
      "▁máy        O\n",
      "▁sam        B-GENERAL\n",
      "▁làm        I-GENERAL\n",
      "▁tệ         I-GENERAL\n",
      "▁quá        I-GENERAL\n",
      "▁không      I-GENERAL\n",
      "▁bằng       I-GENERAL\n",
      "▁mấy        I-GENERAL\n",
      "▁con        I-GENERAL\n",
      "▁tàu        I-GENERAL\n",
      "▁cùng       I-GENERAL\n",
      "▁phân       I-GENERAL\n",
      "_           I-GENERAL\n",
      "kh          I-GENERAL\n",
      "úc          O\n",
      "</s>        I-GENERAL\n",
      "<pad>       O\n"
     ]
    }
   ],
   "source": [
    "def print_sample(dataset, index, k=30): # k will be positive\n",
    "    # print the first k tokens and corresponding labels\n",
    "    for token, label in zip(tokenizer.convert_ids_to_tokens(dataset[index][\"ids\"][:k]), dataset[index][\"targets\"][:k]):\n",
    "        print('{0:10}  {1}'.format(token, id_to_tag[label.item()]))\n",
    "\n",
    "print_sample(train_dataset, 1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create dataloader\n",
    "def create_data_loader(datasets, params):\n",
    "\n",
    "    return DataLoader(\n",
    "        datasets,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "# define dataloader params\n",
    "train_params = {\n",
    "    'batch_size': 16,\n",
    "    'shuffle': True,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "dev_params = {\n",
    "    'batch_size': 8,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "# create dataloader\n",
    "train_dataloader = create_data_loader(train_dataset, train_params)\n",
    "dev_dataloader = create_data_loader(dev_dataset, dev_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model\n",
    "\n",
    "Here we define the model, BertForTokenClassification, and load it with the pretrained weights of \"bert-base-uncased\". The only thing we need to additionally specify is the number of labels (as this will determine the architecture of the classification head).\n",
    "\n",
    "Note that only the base layers are initialized with the pretrained weights. The token classification head of top has just randomly initialized weights, which we will train, together with the pretrained weights, using our labelled dataset. This is also printed as a warning when you run the code cell below.\n",
    "\n",
    "Then, we move the model to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\zac2023\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at nguyenvulebinh/vi-mrc-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=21, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(MRC_PATH,\n",
    "                                                   num_labels=len(tag_to_id),\n",
    "                                                   id2label=id_to_tag,\n",
    "                                                   label2id=tag_to_id,\n",
    "                                                   use_auth_token=AUTH_TOKEN)\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function for NER\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets, masks):\n",
    "    \"\"\"\n",
    "    outputs: (batch_size, seq_len, num_labels)\n",
    "    targets: (batch_size, seq_len)\n",
    "    masks: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # reshape to (batch_size * seq_len, num_labels)\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # ignore padded tokens\n",
    "    masks = masks.view(-1)\n",
    "    logits = logits[masks == 1]\n",
    "    targets = targets[masks == 1]\n",
    "\n",
    "    # compute cross entropy loss\n",
    "    return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "def accuracy_f1(logits, targets, masks):\n",
    "    \"\"\"\n",
    "    outputs: (batch_size, seq_len, num_labels)\n",
    "    targets: (batch_size, seq_len)\n",
    "    masks: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # reshape to (batch_size * seq_len, num_labels)\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # ignore padded tokens\n",
    "    masks = masks.view(-1)\n",
    "    logits = logits[masks == 1]\n",
    "    targets = targets[masks == 1]\n",
    "\n",
    "    # compute accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy_score(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "    # compute f1 score\n",
    "    f1 = f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample input\n",
    "model.train()\n",
    "sample = next(iter(train_dataloader))\n",
    "print(sample['ids'].shape)\n",
    "print(sample['mask'].shape)\n",
    "print(sample['targets'].shape)\n",
    "\n",
    "out = model(sample['ids'].to(device), sample['mask'].to(device))\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.006666666666666667, 0.004806066161403662)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_f1(out.logits, sample['targets'].to(device), sample['mask'].to(device))\n",
    "# loss_fn(out.logits, sample['targets'].to(device), sample['mask'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import torch.optim as optim\n",
    "\n",
    "def optimizer_scheduler(model, num_train_steps, lr=5e-5):\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", 'LayerNorm.bias', \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.001,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "    opt = optim.AdamW(optimizer_parameters, lr=lr)\n",
    "    sch = get_linear_schedule_with_warmup(\n",
    "        opt,\n",
    "        num_warmup_steps=int(0.05*num_train_steps),\n",
    "        num_training_steps=num_train_steps,\n",
    "        last_epoch=-1,\n",
    "    )\n",
    "    return opt, sch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prarams for training\n",
    "epochs = 10\n",
    "accumulation_steps = 4\n",
    "\n",
    "total_steps = len(train_dataloader) * epochs // accumulation_steps\n",
    "optimizer, scheduler = optimizer_scheduler(model, total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "\n",
    "    loss, accuracy, f1_score = 0, 0, 0\n",
    "    train_steps = len(train_dataloader) // accumulation_steps\n",
    "    model.train()\n",
    "\n",
    "    for step, batch in tqdm(enumerate(train_dataloader), total=train_steps):\n",
    "\n",
    "        ids = batch['ids'].to(device, dtype=torch.long)\n",
    "        mask = batch['mask'].to(device, dtype=torch.long)\n",
    "        targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(ids, mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # compute loss\n",
    "        batch_loss = loss_fn(logits, targets, mask)\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Batch loss of epoch {epoch} at step {step}: {batch_loss.item()}\")\n",
    "        \n",
    "        # backward pass\n",
    "        batch_loss /= accumulation_steps\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        # compute accuracy and f1 score\n",
    "        batch_acc, batch_f1 = accuracy_f1(logits, targets, mask)\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        accuracy += batch_acc.item()\n",
    "        f1_score += batch_f1     \n",
    "\n",
    "    return loss / train_steps, accuracy / train_steps, f1_score / train_steps\n",
    "\n",
    "def evaluate(epoch):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, accuracy, f1_score = 0, 0, 0\n",
    "        dev_steps = len(dev_dataloader)\n",
    "        model.eval()\n",
    "        for step, batch in tqdm(enumerate(dev_dataloader), total=dev_steps):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(ids, mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # compute loss\n",
    "            batch_loss = loss_fn(logits, targets, mask)\n",
    "\n",
    "            # compute accuracy and f1 score\n",
    "            batch_acc, batch_f1 = accuracy_f1(logits, targets, mask)\n",
    "\n",
    "            loss += batch_loss.item()\n",
    "            accuracy += batch_acc.item()\n",
    "            f1_score += batch_f1     \n",
    "\n",
    "    return loss / dev_steps, accuracy / dev_steps, f1_score / dev_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/119 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch loss of epoch 0 at step 0: 3.228870153427124\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1} of {epochs}\")\n",
    "    train_loss, train_acc, train_f1 = train(epoch)\n",
    "    print(f\"Train loss: {train_loss}, Train accuracy: {train_acc}, Train F1 score: {train_f1}\")\n",
    "    dev_loss, dev_acc, dev_f1 = evaluate(epoch)\n",
    "    print(f\"Dev loss: {dev_loss}, Dev accuracy: {dev_acc}, Dev F1 score: {dev_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save_pretrained('model/span_detection_bert_base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zac2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
