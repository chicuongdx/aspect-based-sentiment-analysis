{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_PATH = 'data/process_data_final/train.json'\n",
    "# DEV_PATH = 'data/process_data_final/dev.json'\n",
    "# TEST_PATH = 'data/process_data_final/test.json'\n",
    "\n",
    "TRAIN_PATH = 'data/span_detection_datasets_split_word_IOB/train.jsonl'\n",
    "DEV_PATH = 'data/span_detection_datasets_split_word_IOB/dev.jsonl'\n",
    "TEST_PATH = 'data/span_detection_datasets_split_word_IOB/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function read jsonl file as dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = read_jsonl_to_dataframe(TRAIN_PATH)\n",
    "# dev_df = pd.read_json(DEV_PATH)\n",
    "# test_df = pd.read_json(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[pin, sài_tầm, 50, h, cho, pin, 100, 100, came...</td>\n",
       "      <td>[B-BATTERY, I-BATTERY, I-BATTERY, I-BATTERY, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[lag, va, hao, pin, là, cái, tóm_tắt, về, máy,...</td>\n",
       "      <td>[B-PERFORMANCE, O, B-BATTERY, I-BATTERY, O, O,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tất_cả, đều, ổn, ngoại_trừ, lúc, máy, nóng, l...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[ok, mua, máy, ở, tgdd, chính_sách, đổi, trả, ...</td>\n",
       "      <td>[O, O, O, O, O, B-SER&amp;ACC, I-SER&amp;ACC, I-SER&amp;AC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[kiểu_dáng, thì, đẹp, cầm_chắc, tay, nhưn, loa...</td>\n",
       "      <td>[B-DESIGN, I-DESIGN, I-DESIGN, I-DESIGN, I-DES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7620</th>\n",
       "      <td>[mình, vừa, mua, máy, hôm_nay, bản, màu, ghi, ...</td>\n",
       "      <td>[O, O, O, O, O, B-DESIGN, I-DESIGN, I-DESIGN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7621</th>\n",
       "      <td>[máy, mua, đc, 2, thang, bị, lỗi, camera, trướ...</td>\n",
       "      <td>[O, O, O, O, O, B-CAMERA, I-CAMERA, I-CAMERA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7622</th>\n",
       "      <td>[máy, màn_hình, cứ, tự, sáng, liên_tục, dù, ch...</td>\n",
       "      <td>[B-FEATURES, I-FEATURES, I-FEATURES, I-FEATURE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7623</th>\n",
       "      <td>[sau, gần, một, tuần, sử_dụng, máy, cảm_thấy, ...</td>\n",
       "      <td>[O, O, O, O, O, B-GENERAL, I-GENERAL, I-GENERA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7624</th>\n",
       "      <td>[máy, bắt, wifi, 4g, quá, kém, thường_xuyên, b...</td>\n",
       "      <td>[B-FEATURES, I-FEATURES, I-FEATURES, I-FEATURE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7625 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     [pin, sài_tầm, 50, h, cho, pin, 100, 100, came...   \n",
       "1     [lag, va, hao, pin, là, cái, tóm_tắt, về, máy,...   \n",
       "2     [tất_cả, đều, ổn, ngoại_trừ, lúc, máy, nóng, l...   \n",
       "3     [ok, mua, máy, ở, tgdd, chính_sách, đổi, trả, ...   \n",
       "4     [kiểu_dáng, thì, đẹp, cầm_chắc, tay, nhưn, loa...   \n",
       "...                                                 ...   \n",
       "7620  [mình, vừa, mua, máy, hôm_nay, bản, màu, ghi, ...   \n",
       "7621  [máy, mua, đc, 2, thang, bị, lỗi, camera, trướ...   \n",
       "7622  [máy, màn_hình, cứ, tự, sáng, liên_tục, dù, ch...   \n",
       "7623  [sau, gần, một, tuần, sử_dụng, máy, cảm_thấy, ...   \n",
       "7624  [máy, bắt, wifi, 4g, quá, kém, thường_xuyên, b...   \n",
       "\n",
       "                                                 labels  \n",
       "0     [B-BATTERY, I-BATTERY, I-BATTERY, I-BATTERY, I...  \n",
       "1     [B-PERFORMANCE, O, B-BATTERY, I-BATTERY, O, O,...  \n",
       "2     [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "3     [O, O, O, O, O, B-SER&ACC, I-SER&ACC, I-SER&AC...  \n",
       "4     [B-DESIGN, I-DESIGN, I-DESIGN, I-DESIGN, I-DES...  \n",
       "...                                                 ...  \n",
       "7620  [O, O, O, O, O, B-DESIGN, I-DESIGN, I-DESIGN, ...  \n",
       "7621  [O, O, O, O, O, B-CAMERA, I-CAMERA, I-CAMERA, ...  \n",
       "7622  [B-FEATURES, I-FEATURES, I-FEATURES, I-FEATURE...  \n",
       "7623  [O, O, O, O, O, B-GENERAL, I-GENERAL, I-GENERA...  \n",
       "7624  [B-FEATURES, I-FEATURES, I-FEATURES, I-FEATURE...  \n",
       "\n",
       "[7625 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1714: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # load the tokenizer for the model BiLSTM\n",
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('nguyenvulebinh/vi-mrc-large', use_auth_token=AUTH_TOKEN)\n",
    "\n",
    "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# def tokenize_sentences(sentences, tokenizer, max_length=384):\n",
    "#     input_ids = []\n",
    "#     attention_masks = []\n",
    "\n",
    "#     # For every sentence...\n",
    "#     for sent in sentences:\n",
    "#         encoded_dict = tokenizer.encode_plus(\n",
    "#                             sent,                      # Sentence to encode.\n",
    "#                             add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "#                             max_length = max_length,           # Pad & truncate all sentences.\n",
    "#                             pad_to_max_length = True,\n",
    "#                             return_attention_mask = True,   # Construct attn. masks.\n",
    "#                             return_tensors = 'pt',\n",
    "#                             truncation=True\n",
    "#                        )\n",
    "        \n",
    "#         # Add the encoded sentence to the list.    \n",
    "#         input_ids.append(encoded_dict['input_ids'])\n",
    "        \n",
    "#         # And its attention mask (simply differentiates padding from non-padding).\n",
    "#         attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "#     return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df['text'].values\n",
    "# dev_sentences = dev_df['text'].values\n",
    "# test_sentences = test_df['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\udemy\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2393: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
    "# train_input_ids, train_attention_masks = tokenize_sentences(train_sentences, tokenizer)\n",
    "# dev_input_ids, dev_attention_masks = tokenize_sentences(dev_sentences, tokenizer)\n",
    "# test_input_ids, test_attention_masks = tokenize_sentences(test_sentences, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the data\n",
    "# import torch\n",
    "\n",
    "# torch.save(train_input_ids, 'data/tokenizer_data/train_input_ids.pt')\n",
    "# torch.save(train_attention_masks, 'data/tokenizer_data/train_attention_masks.pt')\n",
    "# torch.save(dev_input_ids, 'data/tokenizer_data/dev_input_ids.pt')\n",
    "# torch.save(dev_attention_masks, 'data/tokenizer_data/dev_attention_masks.pt')\n",
    "# torch.save(test_input_ids, 'data/tokenizer_data/test_input_ids.pt')\n",
    "# torch.save(test_attention_masks, 'data/tokenizer_data/test_attention_masks.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create vocab from train data, dev data and test data\n",
    "# vocab = set()\n",
    "# for sent in np.concatenate([train_sentences, dev_sentences, test_sentences]):\n",
    "#     vocab.update(sent.split())\n",
    "# for sent in dev_sentences:\n",
    "#     vocab.update(sent.split())\n",
    "# for sent in test_sentences:\n",
    "#     vocab.update(sent.split())\n",
    "# vocab = list(vocab)\n",
    "# vocab.sort()\n",
    "\n",
    "# # insert special tokens to first index\n",
    "# vocab.insert(0, '<UNK>')\n",
    "# vocab.insert(1, '<PAD>')\n",
    "\n",
    "\n",
    "# import json\n",
    "# # Save vocab as a txt file\n",
    "# with open('data/vocab.txt', 'w', encoding='utf-8') as f:\n",
    "#     for word in vocab:\n",
    "#         f.write(word + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labels process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df['labels'].values\n",
    "# train_dev_labels = dev_df['labels'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag_to_id\n",
    "tag_to_id = {}\n",
    "\n",
    "tag_to_id['O'] = 0\n",
    "for label in train_labels:\n",
    "\n",
    "    for span in label:\n",
    "\n",
    "        if span == 'O':\n",
    "            continue\n",
    "        \n",
    "        aspect = span\n",
    "\n",
    "        if aspect not in tag_to_id:\n",
    "            tag_to_id[aspect] = len(tag_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tag_to_id\n",
    "import json\n",
    "\n",
    "# with open('data/tag_to_id.json', 'w') as f:\n",
    "#     json.dump(tag_to_id, f)\n",
    "\n",
    "with open('data/tag_to_id_bert.json', 'w') as f:\n",
    "    json.dump(tag_to_id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
