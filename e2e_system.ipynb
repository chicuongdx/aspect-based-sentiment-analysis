{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\zac2023\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "TEST_PATH = 'data/span_detection_datasets_split_word_IOB/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function read jsonl file as dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id_bert.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load sentiment_to_id\n",
    "with open('data/sentiment_to_id.json', 'r') as f:\n",
    "    sentiment_to_id = json.load((f))\n",
    "\n",
    "# convert tag_to_id to id_to_tag\n",
    "id_to_tag = {v: k for k, v in tag_to_id.items()}\n",
    "\n",
    "# convert sentiment_to_id to id_to_sentiment\n",
    "id_to_sentiment = {v: k for k, v in sentiment_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test data\n",
    "\n",
    "df_test = read_jsonl_to_dataframe(TEST_PATH)\n",
    "\n",
    "df_test.text = df_test.text.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Span Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = \".env\"\n",
    "\n",
    "try:\n",
    "    with open(env, \"r\") as file:\n",
    "        AUTH_TOKEN = file.read()\n",
    "except FileNotFoundError:\n",
    "    print(f\"The file {env} does not exist.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "MRC_PATH = 'nguyenvulebinh/vi-mrc-base'\n",
    "PRETRAINED_PATH = 'model/span_detection_bert_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MRC_PATH)\n",
    "# load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(PRETRAINED_PATH,\n",
    "                                                   num_labels=len(tag_to_id),\n",
    "                                                   id2label=id_to_tag,\n",
    "                                                   label2id=tag_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 277,469,205\n"
     ]
    }
   ],
   "source": [
    "def num_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def model_architecture(model):\n",
    "    # print all layers and number of parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")\n",
    "\n",
    "print(f\"Number of parameters: {num_params(model):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy_f1(logits, targets, masks):\n",
    "    \"\"\"\n",
    "    outputs: (batch_size, seq_len, num_labels)\n",
    "    targets: (batch_size, seq_len)\n",
    "    masks: (batch_size, seq_len)\n",
    "    \"\"\"\n",
    "    # reshape to (batch_size * seq_len, num_labels)\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # ignore padded tokens\n",
    "    masks = masks.view(-1)\n",
    "    logits = logits[masks == 1]\n",
    "    targets = targets[masks == 1]\n",
    "\n",
    "    # compute accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy_score(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "    # compute f1 score\n",
    "    f1 = f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "\n",
    "    return acc, f1\n",
    "\n",
    "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
    "    \"\"\"\n",
    "    Word piece tokenization makes it difficult to match word labels\n",
    "    back up with individual word pieces. This function tokenizes each\n",
    "    word one at a time so that it is easier to preserve the correct\n",
    "    label for each subword. It is, of course, a bit slower in processing\n",
    "    time, but it will help our model achieve higher accuracy.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    for word, label in zip(sentence.split(), text_labels):\n",
    "\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define special tokens\n",
    "pad_token = tokenizer.pad_token\n",
    "sep_token = tokenizer.sep_token\n",
    "cls_token = tokenizer.cls_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpanDetectionDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=MAX_LEN):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # step 1: tokenize (and adapt corresponding labels)\n",
    "        sentence = self.data.text[index]  \n",
    "        word_labels = self.data.labels[index]  \n",
    "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
    "        \n",
    "        # step 2: add special tokens (and corresponding labels)\n",
    "        tokenized_sentence = [cls_token] + tokenized_sentence + [sep_token] # add special tokens\n",
    "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
    "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
    "\n",
    "        # step 3: truncating/padding\n",
    "        maxlen = self.max_len\n",
    "\n",
    "        if (len(tokenized_sentence) > maxlen):\n",
    "          # truncate\n",
    "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
    "          labels = labels[:maxlen]\n",
    "        else:\n",
    "          # pad\n",
    "          tokenized_sentence = tokenized_sentence + [pad_token for _ in range(maxlen - len(tokenized_sentence))]\n",
    "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
    "\n",
    "        # step 4: obtain the attention mask\n",
    "        attn_mask = [1 if tok != pad_token else 0 for tok in tokenized_sentence]\n",
    "        \n",
    "        # step 5: convert tokens to input ids\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "\n",
    "        label_ids = [tag_to_id[label] for label in labels]\n",
    "        # the following line is deprecated\n",
    "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
    "        \n",
    "        return {\n",
    "              'ids': torch.tensor(ids, dtype=torch.long),\n",
    "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
    "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
    "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SpanDetectionDataset(\u001b[43mdf_test\u001b[49m, tokenizer, max_len\u001b[38;5;241m=\u001b[39mMAX_LEN)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# function to create dataloader\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_data_loader\u001b[39m(datasets, params):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_test' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset = SpanDetectionDataset(df_test, tokenizer, max_len=MAX_LEN)\n",
    "# function to create dataloader\n",
    "def create_data_loader(datasets, params):\n",
    "\n",
    "    return DataLoader(\n",
    "        datasets,\n",
    "        **params\n",
    "    )\n",
    "\n",
    "test_params = {\n",
    "    'batch_size': 16,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "test_loader = create_data_loader(test_dataset, test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loader):\n",
    "    with torch.no_grad():\n",
    "        # model.eval()\n",
    "        steps = len(loader)\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        f1 = 0\n",
    "        for step, batch in tqdm(enumerate(loader), total=steps):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(ids, mask)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # # compute loss\n",
    "            # batch_loss = loss_fn(logits, targets, mask)\n",
    "\n",
    "            # compute accuracy and f1 score\n",
    "            batch_acc, batch_f1 = accuracy_f1(logits, targets, mask)\n",
    "\n",
    "            # loss += batch_loss.item()\n",
    "            acc += batch_acc.item()\n",
    "            f1 += batch_f1    \n",
    "    return loss / steps, acc / steps, f1 / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 137/137 [07:05<00:00,  3.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.757 | Test f1 score: 0.665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "_, test_acc, test_f1 = test(model, test_loader)\n",
    "print(f\"Test accuracy: {test_acc:.3f} | Test f1 score: {test_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "MAX_LEN_SA = 128\n",
    "TEST_PATH_SA = 'data/sentiment_analysis_data/test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRC_PATH = 'nguyenvulebinh/vi-mrc-base'\n",
    "PHOBERT_PATH = 'vinai/phobert-base'\n",
    "PRETRAINED_PATH_SA = 'model/sentiment_analysis_bert_base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_sa = read_jsonl_to_dataframe(TEST_PATH_SA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\zac2023\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:671: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "tokenizer_sa = AutoTokenizer.from_pretrained(PHOBERT_PATH, use_auth_token=AUTH_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=MAX_LEN_SA):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.data.text[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        label = self.data.sentiment[index]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'targets': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_params_sa = {\n",
    "    'batch_size': 16,\n",
    "    'shuffle': False,\n",
    "    'num_workers': 0\n",
    "}\n",
    "\n",
    "test_dataset_sa = SentimentAnalysisDataset(df_test_sa, tokenizer_sa)\n",
    "test_loader_sa = create_data_loader(test_dataset_sa, test_params_sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(258, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sa = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_PATH_SA)\n",
    "\n",
    "model_sa.to(device)\n",
    "model_sa.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def loss_fn(logits, targets):\n",
    "    \"\"\"\n",
    "    outputs: (batch_size, num_labels)\n",
    "    targets: (batch_size,)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # reshape\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # compute cross entropy loss\n",
    "    return nn.CrossEntropyLoss()(logits, targets)\n",
    "\n",
    "def accuracy_f1(logits, targets):\n",
    "    \"\"\"\n",
    "    outputs: (batch_size, num_labels)\n",
    "    targets: (batch_size, 1)\n",
    "    \"\"\"\n",
    "    # reshape to (batch_size * seq_len, num_labels)\n",
    "    logits = logits.view(-1, logits.shape[-1])\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    # compute accuracy\n",
    "    preds = torch.argmax(logits, dim=1)\n",
    "    acc = accuracy_score(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "\n",
    "    # compute f1 score\n",
    "    f1 = f1_score(targets.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "\n",
    "    return acc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sa(model, loader):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        loss, accuracy, f1_score = 0, 0, 0\n",
    "        steps = len(loader)\n",
    "        model.eval()\n",
    "        for step, batch in tqdm(enumerate(loader), total=steps):\n",
    "\n",
    "            ids = batch['ids'].to(device, dtype=torch.long)\n",
    "            mask = batch['mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            targets = batch['targets'].to(device, dtype=torch.long)\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(ids, mask, token_type_ids=token_type_ids)\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # compute accuracy and f1 score\n",
    "            batch_acc, batch_f1 = accuracy_f1(logits, targets)\n",
    "            \n",
    "            accuracy += batch_acc.item()\n",
    "            f1_score += batch_f1     \n",
    "\n",
    "    return loss / steps, accuracy / steps, f1_score / steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [10:33<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.942 | Test f1 score: 0.859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "_, test_acc_sa, test_f1_sa = test_sa(model_sa, test_loader_sa)\n",
    "print(f\"Test accuracy: {test_acc_sa:.3f} | Test f1 score: {test_f1_sa:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End to end model prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Span detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "import regex as re\n",
    "\n",
    "def tokenize(text):\n",
    "    return word_tokenize(text, format=\"text\")\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text) # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text) # remove extra space\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def post_process(text):\n",
    "\n",
    "    text = tokenize(text)\n",
    "    text = preprocess(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end to end prediction function\n",
    "def e2e_span_detection(sentence):\n",
    "\n",
    "    # preprocess sentence\n",
    "    processed_sentence = post_process(sentence)\n",
    "\n",
    "    # tokenize sentence\n",
    "    tokenized_sentence = tokenizer.tokenize(processed_sentence)\n",
    "\n",
    "    # add special tokens\n",
    "    tokenized_sentence = [cls_token] + tokenized_sentence + [sep_token]\n",
    "\n",
    "    # convert tokens to input ids\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "    mask = [1 if tok != pad_token else 0 for tok in tokenized_sentence]\n",
    "\n",
    "    # convert to tensor\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n",
    "\n",
    "    # move to device\n",
    "    ids = ids.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model(ids, mask)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # get predictions\n",
    "    preds = torch.argmax(logits, dim=2).squeeze(0)\n",
    "\n",
    "    # convert to numpy array\n",
    "    preds = preds.detach().cpu().numpy()\n",
    "\n",
    "    # convert to tags\n",
    "    preds = [id_to_tag[pred] for pred in preds]\n",
    "\n",
    "    # get entities\n",
    "    entities = []\n",
    "\n",
    "    for i, pred in enumerate(preds):\n",
    "        if pred != 'O':\n",
    "            if pred.startswith('B-'):\n",
    "                entity = [i, i]\n",
    "                entity.append(pred.split('-')[1])\n",
    "                entities.append(entity)\n",
    "            elif pred.startswith('I-'):\n",
    "                entities[-1][1] = i\n",
    "            else:\n",
    "                print(\"Something wrong\")\n",
    "\n",
    "    # get entities text\n",
    "\n",
    "    entities_text = []\n",
    "\n",
    "    for entity in entities:\n",
    "        start, end, tag = entity\n",
    "        entity_text = tokenized_sentence[start: end+1]\n",
    "        entity_text = tokenizer.convert_tokens_to_string(entity_text)\n",
    "        entities_text.append(entity_text)\n",
    "\n",
    "    return entities_text, entities\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_sentiment_analysis(span):\n",
    "    # tokenize\n",
    "    inputs = tokenizer_sa.encode_plus(\n",
    "        span,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=MAX_LEN_SA,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "\n",
    "    ids = inputs['input_ids']\n",
    "    mask = inputs['attention_mask']\n",
    "    token_type_ids = inputs[\"token_type_ids\"]\n",
    "\n",
    "    # convert to tensor\n",
    "    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = model_sa(ids, mask, token_type_ids=token_type_ids)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # compute probability\n",
    "    probs = nn.functional.softmax(logits, dim=1).squeeze(0)\n",
    "\n",
    "    # get label\n",
    "    label = torch.argmax(probs).item()\n",
    "\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e2e_system(sentence):\n",
    "\n",
    "    # span detection\n",
    "    entities_text, entities = e2e_span_detection(sentence)\n",
    "\n",
    "    # sentiment analysis\n",
    "    sentiments = []\n",
    "    for span in entities_text:\n",
    "        sentiment = e2e_sentiment_analysis(span)\n",
    "        sentiments.append(sentiment)\n",
    "\n",
    "    # combine entities and sentiments\n",
    "    entities_sentiments = []\n",
    "    for entity, sentiment in zip(entities, sentiments):\n",
    "        entities_sentiments.append(f\"{entity[2]}#{id_to_sentiment[sentiment]}\")\n",
    "\n",
    "    # remove duplicate\n",
    "    entities_sentiments = list(set(entities_sentiments))\n",
    "\n",
    "    return entities_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span: sp ổn - Aspect: GENERAL\n",
      "Span: mỗi tội - Aspect: FEATURES\n",
      "Span: vân_tay lúc nhận lúc không - Aspect: FEATURES\n",
      "Span: nhân - Aspect: SER&ACC\n",
      "Span: _ - Aspect: SER&ACC\n",
      "Span: vi - Aspect: SER&ACC\n",
      "Span: ên nhiệt_tình - Aspect: SER&ACC\n",
      "Span: pin trâu cả đêm tụt 1</s> - Aspect: BATTERY\n"
     ]
    }
   ],
   "source": [
    "sample = \"Sp ổn, mỗi tội vân tay lúc nhận lúc không, nhân viên nhiệt tình, pin trâu, cả đêm tụt 1%\"\n",
    "\n",
    "spans, aspects = e2e_span_detection(sample)\n",
    "\n",
    "for span, aspect in zip(spans, aspects):\n",
    "    print(f\"Span: {span} - Aspect: {aspect[2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BATTERY#POSITIVE',\n",
       " 'FEATURES#NEGATIVE',\n",
       " 'SER&ACC#NEGATIVE',\n",
       " 'GENERAL#POSITIVE',\n",
       " 'SER&ACC#POSITIVE']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# end to end sample for sample\n",
    "e2e_system(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zac2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
