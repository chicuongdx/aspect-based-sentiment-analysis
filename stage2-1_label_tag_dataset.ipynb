{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = 'data/process_data_split_words/train.jsonl'\n",
    "TEST_PATH = 'data/process_data_split_words/test.jsonl'\n",
    "DEV_PATH = 'data/process_data_split_words/dev.jsonl'\n",
    "\n",
    "# TRAIN_PATH = 'data/process_data/train.json'\n",
    "# TEST_PATH = 'data/process_data/test.json'\n",
    "# DEV_PATH = 'data/process_data/dev.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function read jsonl file as dataframe\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def read_jsonl_to_dataframe(file_path):\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_obj = json.loads(line)\n",
    "                data.append(json_obj)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Skipping invalid JSON: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = read_jsonl_to_dataframe(TRAIN_PATH)\n",
    "df_test = read_jsonl_to_dataframe(TEST_PATH)\n",
    "df_dev = read_jsonl_to_dataframe(DEV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this process just for aspect extraction\n",
    "\n",
    "df_train.labels = df_train.labels.apply(lambda label: [ap.split('#')[0] if ap != 'O' else ap for ap in label])\n",
    "df_test.labels = df_test.labels.apply(lambda label: [ap.split('#')[0] if ap != 'O' else ap for ap in label])\n",
    "df_dev.labels = df_dev.labels.apply(lambda label: [ap.split('#')[0] if ap != 'O' else ap for ap in label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All data is ok\n"
     ]
    }
   ],
   "source": [
    "def print_data(df, index):\n",
    "    print(f'Index: {index}')\n",
    "\n",
    "    columns = df.columns\n",
    "\n",
    "    for column in columns:\n",
    "        print(f'{column}: {df[column][index]}')\n",
    "\n",
    "    print('\\n')\n",
    "    print(\"\\n=============================================================================================\\n\")\n",
    "\n",
    "def print_df(df):\n",
    "    for i in range(len(df)):\n",
    "        print_data(df, i)\n",
    "        \n",
    "# check data\n",
    "def check_data():\n",
    "    check_train = len(df_train[df_train.text.map(len) != df_train.labels.map(len)]) == 0\n",
    "    check_test = len(df_test[df_test.text.map(len) != df_test.labels.map(len)]) == 0\n",
    "    check_dev = len(df_dev[df_dev.text.map(len) != df_dev.labels.map(len)]) == 0\n",
    "\n",
    "    if check_train and check_test and check_dev:\n",
    "        print('All data is ok')\n",
    "    else:\n",
    "        print('Data is not ok at:')\n",
    "        if not check_train:\n",
    "            print('Train data')\n",
    "        if not check_test:\n",
    "            print('Test data')\n",
    "        if not check_dev:\n",
    "            print('Dev data')\n",
    "\n",
    "check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_IOB_format_and_decay_to_token(text, labels):\n",
    "\n",
    "    iob_labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for span, label in zip(text, labels):\n",
    "        current_tokens = span.split(' ')\n",
    "        tokens.extend(current_tokens)\n",
    "        if label == 'O':\n",
    "            iob_labels.extend(['O'] * len(current_tokens))\n",
    "        else:\n",
    "            iob_labels.append(f'B-{label}')\n",
    "            iob_labels.extend([f'I-{label}'] * (len(current_tokens) - 1))\n",
    "    # return iob_labels\n",
    "    return tokens, iob_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels in the training data, dev data, and test data to IOB format and decay to token\n",
    "df_train.text, df_train.labels = zip(*df_train.apply(lambda row: convert_to_IOB_format_and_decay_to_token(row['text'], row['labels']), axis=1))\n",
    "df_test.text, df_test.labels = zip(*df_test.apply(lambda row: convert_to_IOB_format_and_decay_to_token(row['text'], row['labels']), axis=1))\n",
    "df_dev.text, df_dev.labels = zip(*df_dev.apply(lambda row: convert_to_IOB_format_and_decay_to_token(row['text'], row['labels']), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "import os\n",
    "\n",
    "def save_data_to_jsonl(df, folder, filename):\n",
    "    if not os.path.exists(folder):\n",
    "        os.mkdir(folder)\n",
    "\n",
    "\n",
    "    with open(os.path.join(folder, filename), 'w', encoding='utf-8') as file:\n",
    "\n",
    "        # get all name of columns\n",
    "        columns = df.columns\n",
    "        for index, row in df.iterrows():\n",
    "            json_obj = {}\n",
    "            for column in columns:\n",
    "                json_obj[column] = row[column]\n",
    "            json.dump(json_obj, file, ensure_ascii=False)\n",
    "            file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file jsonl\n",
    "save_data_to_jsonl(df_train, 'data/span_detection_datasets_split_word_IOB', 'train.jsonl')\n",
    "save_data_to_jsonl(df_test, 'data/span_detection_datasets_split_word_IOB', 'test.jsonl')\n",
    "save_data_to_jsonl(df_dev, 'data/span_detection_datasets_split_word_IOB', 'dev.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lag', 'va', 'hao', 'pin', 'là', 'cái', 'tóm_tắt', 'về', 'máy', 'sam', 'làm', 'tệ', 'quá', 'không', 'bằng', 'mấy', 'con', 'tàu', 'cùng', 'phân_khúc']\n",
      "['B-PERFORMANCE', 'O', 'B-BATTERY', 'I-BATTERY', 'O', 'O', 'O', 'O', 'O', 'B-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL', 'I-GENERAL']\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print(df_train.iloc[idx]['text'])\n",
    "print(df_train.iloc[idx]['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
