{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from bilstm_crf import build_bilstm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')\n",
    "\n",
    "# load vocab\n",
    "# with open('data/vocab.txt', 'r') as f:\n",
    "#     vocab = f.read().split('\\n')\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/span_detection_datasets_IOB/train.json'\n",
    "DEV_PATH = 'data/span_detection_datasets_IOB/dev.json'\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "train_sentences = list(train_data['text'].values())\n",
    "dev_sentences = list(dev_data['text'].values())\n",
    "\n",
    "train_labels = list(train_data['labels'].values())\n",
    "dev_labels = list(dev_data['labels'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "tokenizer = TextVectorization(pad_to_max_tokens=True,\n",
    "                              output_sequence_length=256,\n",
    "                              output_mode='int',\n",
    "                              max_tokens=10000)\n",
    "tokenizer.adapt(train_sentences + dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab and save it to file vocab.txt\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "vocab = vocab[2:]\n",
    "vocab = ['<UNK>', '<PAD>'] + vocab\n",
    "with open('data/vocab.txt', 'w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokenized and dev_tokenized are numpy array with padding\n",
    "train_tokenized = tokenizer(np.array([[s] for s in train_sentences])).numpy()\n",
    "dev_tokenized = tokenizer(np.array([[s] for s in dev_sentences])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# Convert labels to ids\n",
    "# labels = [[start, end, tag], ...]\n",
    "def convert_labels_to_ids(label, tag_to_id, max_len=256):\n",
    "    ids = [int(tag_to_id[tag]) for tag in label]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [int(tag_to_id['<PAD>'])] * (max_len - len(ids))\n",
    "        \n",
    "    return np.array(ids, dtype=np.int32)\n",
    "\n",
    "train_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in train_labels], dtype=np.int32)\n",
    "dev_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in dev_labels], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "train_labels_one_hot_encoding = tf.one_hot(train_labels_encoding, len(tag_to_id))\n",
    "dev_labels_one_hotencoding = tf.one_hot(dev_labels_encoding, len(tag_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader tensorflow\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tokenized, train_labels_encoding))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_tokenized, dev_labels_encoding))\n",
    "dev_dataset = dev_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\absa\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(Model):\n",
    "    def __init__(self, vocab_size, max_len, n_tags, embedding_matrix=None, embedding_dim=None, unit='lstm', num_units=100, dropout=0.1, recurrent_dropout=0.1):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "        if embedding_matrix is not None and embedding_dim is not None:\n",
    "            raise ValueError('Cannot provide both an embedding matrix and an embedding dimension.')\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[-1], input_length=max_len, mask_zero=True, weights=[embedding_matrix], trainable=False)\n",
    "        elif embedding_dim is not None:\n",
    "            self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True, embeddings_initializer='uniform')\n",
    "        else:\n",
    "            raise ValueError('Must provide either an embedding matrix or an embedding dimension.')\n",
    "\n",
    "        if unit == 'lstm':\n",
    "            self.lstm = layers.Bidirectional(layers.LSTM(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        elif unit == 'gru':\n",
    "            self.lstm = layers.Bidirectional(layers.GRU(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        elif unit == 'rnn':\n",
    "            self.lstm = layers.Bidirectional(layers.SimpleRNN(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        else:\n",
    "            raise ValueError('Invalid unit type. Must be one of lstm, gru, or rnn.')\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.time_distributed = layers.TimeDistributed(layers.Dense(n_tags, activation=\"relu\"))\n",
    "        \n",
    "        # use fully connected layer instead of CRF (batch_size, max_len) softmax\n",
    "        # self.fc = layers.Dense(n_tags, activation=\"softmax\")\n",
    "        self.crf = CRF(units=n_tags)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.time_distributed(x)\n",
    "        # x = self.fc(x)\n",
    "        x = self.crf(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        x = layers.Input(shape=(256,))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "VOCAB_SIZE = len(vocab)\n",
    "TAG_SIZE = len(tag_to_id)\n",
    "UNITS = 100\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 256, 300)          2420400   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 256, 200)          320800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256, 200)          0         \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 256, 22)           4422      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 256),             1034      \n",
      "                              (None, 256, 22),                   \n",
      "                              (None,),                           \n",
      "                              (22, 22)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2746656 (10.48 MB)\n",
      "Trainable params: 2746656 (10.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(VOCAB_SIZE, MAX_LEN, TAG_SIZE, embedding_dim=EMBEDDING_DIM, num_units=UNITS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2100, in categorical_crossentropy\n        label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n    TypeError: Expected int32, but got 0.1 of type 'float'.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\CLOUDX\\Courses\\nlp\\aspect-based-sentiment-analysis\\training_stage1_train_span_detection.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/training_stage1_train_span_detection.ipynb#X25sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/training_stage1_train_span_detection.ipynb#X25sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m EPOCHS \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/CLOUDX/Courses/nlp/aspect-based-sentiment-analysis/training_stage1_train_span_detection.ipynb#X25sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(train_dataset, epochs\u001b[39m=\u001b[39;49mEPOCHS, validation_data\u001b[39m=\u001b[39;49mdev_dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileo6dx75ku.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: in user code:\n\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"C:\\Users\\Hii\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\losses.py\", line 2100, in categorical_crossentropy\n        label_smoothing = tf.convert_to_tensor(label_smoothing, dtype=y_pred.dtype)\n\n    TypeError: Expected int32, but got 0.1 of type 'float'.\n"
     ]
    }
   ],
   "source": [
    "# compile model\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "metric_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "# metric_f1 = tf.keras.metrics.F1Score(average='micro')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric_acc])\n",
    "\n",
    "# train model\n",
    "EPOCHS = 10\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model/span_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
