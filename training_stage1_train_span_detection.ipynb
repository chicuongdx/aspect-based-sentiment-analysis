{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# from bilstm_crf import build_bilstm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load embedding\n",
    "# embedding_maxtrix = np.load('embedding/embedding_matrix.npy')\n",
    "\n",
    "# load vocab\n",
    "# with open('data/vocab.txt', 'r') as f:\n",
    "#     vocab = f.read().split('\\n')\n",
    "\n",
    "# load tag_to_id\n",
    "with open('data/tag_to_id.json', 'r') as f:\n",
    "    tag_to_id = json.load((f))\n",
    "\n",
    "# load train and dev data\n",
    "TRAIN_PATH = 'data/span_detection_datasets_IOB/train.json'\n",
    "DEV_PATH = 'data/span_detection_datasets_IOB/dev.json'\n",
    "\n",
    "with open(TRAIN_PATH, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "with open(DEV_PATH, 'r') as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "train_sentences = list(train_data['text'].values())\n",
    "dev_sentences = list(dev_data['text'].values())\n",
    "\n",
    "train_labels = list(train_data['labels'].values())\n",
    "dev_labels = list(dev_data['labels'].values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TextVectorization\n",
    "\n",
    "tokenizer = TextVectorization(pad_to_max_tokens=True,\n",
    "                              output_sequence_length=256,\n",
    "                              output_mode='int',\n",
    "                              max_tokens=10000)\n",
    "tokenizer.adapt(train_sentences + dev_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab and save it to file vocab.txt\n",
    "vocab = tokenizer.get_vocabulary()\n",
    "vocab = vocab[2:]\n",
    "vocab = ['<UNK>', '<PAD>'] + vocab\n",
    "with open('data/vocab.txt', 'w') as f:\n",
    "    f.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_tokenized and dev_tokenized are numpy array with padding\n",
    "train_tokenized = tokenizer(np.array([[s] for s in train_sentences])).numpy()\n",
    "dev_tokenized = tokenizer(np.array([[s] for s in dev_sentences])).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to ids\n",
    "# labels = [[start, end, tag], ...]\n",
    "def convert_labels_to_ids(label, tag_to_id, max_len=256):\n",
    "    ids = [int(tag_to_id[tag]) for tag in label]\n",
    "\n",
    "    if len(ids) < max_len:\n",
    "        ids += [int(tag_to_id['<PAD>'])] * (max_len - len(ids))\n",
    "        \n",
    "    return np.array(ids, dtype=np.int32)\n",
    "\n",
    "train_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in train_labels], dtype=np.int32)\n",
    "dev_labels_encoding = np.array([convert_labels_to_ids(label, tag_to_id) for label in dev_labels], dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding\n",
    "train_labels_one_hot_encoding = tf.one_hot(train_labels_encoding, len(tag_to_id))\n",
    "dev_labels_one_hotencoding = tf.one_hot(dev_labels_encoding, len(tag_to_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data loader tensorflow\n",
    "BATCH_SIZE = 4\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_tokenized, train_labels_encoding))\n",
    "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dev_dataset = tf.data.Dataset.from_tensor_slices((dev_tokenized, dev_labels_encoding))\n",
    "dev_dataset = dev_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hii\\miniconda3\\envs\\absa\\Lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "from tensorflow_addons.layers import CRF\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.metrics import F1Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM_CRF(Model):\n",
    "    def __init__(self, vocab_size, max_len, n_tags, embedding_matrix=None, embedding_dim=None, unit='lstm', num_units=100, dropout=0.1, recurrent_dropout=0.1):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "        if embedding_matrix is not None and embedding_dim is not None:\n",
    "            raise ValueError('Cannot provide both an embedding matrix and an embedding dimension.')\n",
    "\n",
    "        if embedding_matrix is not None:\n",
    "            self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_matrix.shape[-1], input_length=max_len, mask_zero=True, weights=[embedding_matrix], trainable=False)\n",
    "        elif embedding_dim is not None:\n",
    "            self.embedding = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len, mask_zero=True, embeddings_initializer='uniform')\n",
    "        else:\n",
    "            raise ValueError('Must provide either an embedding matrix or an embedding dimension.')\n",
    "\n",
    "        if unit == 'lstm':\n",
    "            self.lstm = layers.Bidirectional(layers.LSTM(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        elif unit == 'gru':\n",
    "            self.lstm = layers.Bidirectional(layers.GRU(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        elif unit == 'rnn':\n",
    "            self.lstm = layers.Bidirectional(layers.SimpleRNN(units=num_units, return_sequences=True, dropout=recurrent_dropout))\n",
    "        else:\n",
    "            raise ValueError('Invalid unit type. Must be one of lstm, gru, or rnn.')\n",
    "        \n",
    "        self.dropout = layers.Dropout(dropout)\n",
    "        self.time_distributed = layers.TimeDistributed(layers.Dense(n_tags, activation=\"relu\"))\n",
    "        \n",
    "        # use fully connected layer instead of CRF (batch_size, max_len) softmax\n",
    "        # self.fc = layers.Dense(n_tags, activation=\"softmax\")\n",
    "        self.crf = CRF(units=n_tags)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \n",
    "        x = self.embedding(inputs)\n",
    "        x = self.lstm(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        x = self.time_distributed(x)\n",
    "        # x = self.fc(x)\n",
    "        x = self.crf(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def summary(self):\n",
    "        x = layers.Input(shape=(256,))\n",
    "        model = Model(inputs=[x], outputs=self.call(x))\n",
    "        return model.summary()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_LEN = 256\n",
    "VOCAB_SIZE = len(vocab)\n",
    "TAG_SIZE = len(tag_to_id)\n",
    "UNITS = 100\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 256)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 256, 300)          2420400   \n",
      "                                                                 \n",
      " bidirectional (Bidirection  (None, 256, 200)          320800    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256, 200)          0         \n",
      "                                                                 \n",
      " time_distributed (TimeDist  (None, 256, 22)           4422      \n",
      " ributed)                                                        \n",
      "                                                                 \n",
      " crf (CRF)                   [(None, 256),             1034      \n",
      "                              (None, 256, 22),                   \n",
      "                              (None,),                           \n",
      "                              (22, 22)]                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2746656 (10.48 MB)\n",
      "Trainable params: 2746656 (10.48 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = BiLSTM_CRF(VOCAB_SIZE, MAX_LEN, TAG_SIZE, embedding_dim=EMBEDDING_DIM, num_units=UNITS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "metric_acc = tf.keras.metrics.CategoricalAccuracy()\n",
    "# metric_f1 = tf.keras.metrics.F1Score(average='micro')\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric_acc])\n",
    "\n",
    "# train model\n",
    "EPOCHS = 10\n",
    "history = model.fit(train_dataset, epochs=EPOCHS, validation_data=dev_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss and accuracy of train and dev in one figure\n",
    "def plot_history(history):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    axs[0].plot(history.history['loss'])\n",
    "    axs[0].plot(history.history['val_loss'])\n",
    "    axs[0].set_title('Model loss')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].set_xlabel('Epoch')\n",
    "    axs[0].legend(['Train', 'Val'], loc='upper right')\n",
    "\n",
    "    axs[1].plot(history.history['categorical_accuracy'])\n",
    "    axs[1].plot(history.history['val_categorical_accuracy'])\n",
    "    axs[1].set_title('Model accuracy')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].legend(['Train', 'Val'], loc='upper right')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "model.save('model/span_detection_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
